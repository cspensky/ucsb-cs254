%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured General Purpose Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{forest}
\usepackage{float}
\usepackage{listings}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#3} % Assignment title
\newcommand{\hmwkDueDate}{Friday,\ June\ 10,\ 2016} % Due date
\newcommand{\hmwkClass}{CS\ 254} % Course/class
\newcommand{\hmwkClassInstructor}{Prof. Tim Sherwood} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Chad Spensky} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle
\newpage

\input{macros}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC
%
%\newpage
%\tableofcontents
%\newpage

% To have just one problem per page, simply put a \clearpage after each problem

\section{Streams (3pts)}
Since the stream buffers discussed in the paper work as FIFOs and only compare/pop from the front, we would need at least 2 stream buffers, and they would only be useful for array \textbf{b}, as it is the only one being accesses sequentially.  However, 1001 stream buffers would be able to cache all of the indices for c, and the currently accessed one for b, fully optimizing our accesses.  


\section{Thread Level Parallelism (3pts each)}

\subsection{F}
The IPC for any one thread is simply the number of instructions divided by the number of cycles taken for the loop. 
More precisely,
\begin{eqnarray*}
IPC &= \frac{7~ins}{2*E[load~cycles] +5~cycles}\\
IPC &= \frac{7~ins}{(.3^2*2+11*(.3*.7)*2+.7^2*20) +5~cycles} \approx .357~ins/cycle
\end{eqnarray*}

However, all of the threads must wait for the longest running thread.  Thus we must look at the probability that all threads have no cache misses, at least on thread having a single cache miss, or at least one thread missing both times.
\begin{eqnarray*}
IPC =& \frac{7~ins}{2*E[load~cycles] +5~cycles}\\
IPC =& \frac{7~ins}{16*[2*\Pr(no~misses) + 
11*\Pr(at~least~one~miss,~but~none~have~2) + 
20*\Pr(at~least~one~has~2~misses) +5]~cycles}\\
IPC =& \frac{16*7~ins}{16*[2*.3^{2*16}+11*(1 - .3^{2*16} - (.7*.7)^{2*16}*\sum_{k=1}^{16}\binom{n}{k})+20*(.7*.7)^{2*16}*\sum_{k=1}^{16}\binom{n}{k} + 5]~cycles} \approx 0.24~ins/cycle
\end{eqnarray*}

\subsection{G}
In this case, since the only case we really need only look at this $2x$ thread since the only case that it is not the longest running, and thus determining the number of cycles  for every other thread, is which it has no cache misses.
\begin{eqnarray*}
IPC &= \frac{7*16~ins}{16*[.3^2*(4+E[cycles for other 15 threads])+22*(.3*.7)*2+.7^2*40) +10]~cycles} \approx .072~ins/cycle
\end{eqnarray*}

\subsection{H}
In this case, with the $2x$ thread still in the picture, a close approximation would be to take the answer from F with 15 threads, and add the expected cycles for the longer thread.
\begin{eqnarray*}
IPC &= \frac{7*16~ins}{E[cycles for other 15 threads] + (.3^2*4+22*(.3*.7)*2+.7^2*40) +10)~cycles} \approx .167~ins/cycle
\end{eqnarray*}

\section{Cache Behavior (2pts each)}
\subsection{Q1}
This oddity could be seen if the program was executing in a loop and the number in instructions was twice the size of the cache.  In this case, the least-recently-used scheme would keep removing instructions from the cache before they were needed.  However in the direct mapped cache, it's possible for some of the instructions to remain in the cache, and provide at least some cache hits.

\subsection{Q2}
\subsubsection{A}
Any data that enters the cache, but is never used, is essentially wasted.  Caching only helps for instructions that are prefetched, or used repeatedly.  

\subsubsection{B}
You would need to know wether or not the memory will be used before it is flushed.  While this seems like an unreasonable assumption, some educated guesses could be made (e.g., if there is never a  load from a data memory region).

\section{Transactional Memory (3pts each)}
\subsection{A}
When a transaction commits, it needs to ensure that no other transactions have modified or used any of the data that it also used.  If this check is done at cache-line granularity, data being flushed from the cache could trigger an unnecessary rollback.

\subsection{B}
Insert graph here

\subsection{C}
Insert graph here

\section{Memory Consistency (3pts each)}
\subsection{A}
The problem with V3.0 in the sequential consistency model is that at the beginning, numerous threads could all see my\_world as NULL (e.g., while the other threads are mallocing etc.) and all be waiting for the lock, and then each malloc and initialize their own object.

\subsection{B}
Because there are no synchronization calls being made (assuming locking is just locking memory), it is possible for the lock to be released before the my\_world variable is actually set an propagated, causing other threads to enter the critical code segment again, after the class was already initialized.

\end{document}